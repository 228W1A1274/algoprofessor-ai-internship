{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Day 15 â€” Advanced RAG Evaluation Notebook\n",
    "\n",
    "**Goal:** Systematically compare HyDE, Self-Query, Multi-Query, and Graph RAG  \n",
    "**Framework:** [RAGAS](https://github.com/explodinggradients/ragas) â€” industry-standard RAG evaluation metrics  \n",
    "\n",
    "---\n",
    "### Metrics We'll Measure\n",
    "\n",
    "| Metric | What it measures | Ideal value |\n",
    "|--------|-----------------|-------------|\n",
    "| **Faithfulness** | Is the answer grounded in retrieved context? (no hallucinations) | > 0.85 |\n",
    "| **Answer Relevancy** | Does the answer address the question? | > 0.80 |\n",
    "| **Context Precision** | Are retrieved chunks actually useful? | > 0.75 |\n",
    "| **Context Recall** | Did we retrieve all relevant chunks? | > 0.70 |\n",
    "| **Latency** | Time to first token & total response time | < 3s TTFT |\n",
    "\n",
    "---\n",
    "**Interview tip:** When asked 'how do you evaluate a RAG system?', cite these metrics and explain the hallucination vs. retrieval tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 0. INSTALL DEPENDENCIES (run once) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# !pip install ragas llama-index llama-index-llms-openai llama-index-embeddings-openai \\\n",
    "#             pandas matplotlib seaborn python-dotenv tqdm nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 1. IMPORTS & CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()  # Required to run async code in Jupyter\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv('OPENAI_API_KEY'), 'Set OPENAI_API_KEY in .env!'\n",
    "\n",
    "# LlamaIndex\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0)\n",
    "Settings.embed_model = OpenAIEmbedding(model='text-embedding-3-small')\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "print('âœ… Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 2. GROUND-TRUTH EVALUATION DATASET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# In production: source these from domain experts or label studio.\n",
    "# Each entry: question | ground_truth_answer | relevant_document_ids\n",
    "\n",
    "EVAL_QUESTIONS = [\n",
    "    {\n",
    "        'question': 'What is HyDE and why does it improve retrieval?',\n",
    "        'ground_truth': 'HyDE (Hypothetical Document Embeddings) improves retrieval by generating '\n",
    "                        'a hypothetical answer to the query, then embedding that answer instead of '\n",
    "                        'the query itself. This reduces the semantic gap between question and document embeddings.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'How does multi-query retrieval handle duplicate results?',\n",
    "        'ground_truth': 'Multi-query retrieval uses Reciprocal Rank Fusion (RRF) to deduplicate and '\n",
    "                        'rerank results from multiple query variants. Nodes appearing in multiple result '\n",
    "                        'sets receive higher scores.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the difference between vector RAG and graph RAG?',\n",
    "        'ground_truth': 'Vector RAG uses semantic similarity search to find relevant chunks. '\n",
    "                        'Graph RAG builds a knowledge graph of entities and relations, enabling '\n",
    "                        'multi-hop reasoning across connected concepts.',\n",
    "    },\n",
    "    {\n",
    "        'question': 'Why use SSE for streaming RAG responses?',\n",
    "        'ground_truth': 'Server-Sent Events (SSE) is an HTTP-native, one-way push protocol that '\n",
    "                        'efficiently streams LLM tokens to clients. It reduces time-to-first-token '\n",
    "                        'and improves perceived user experience.',\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f'Evaluation dataset: {len(EVAL_QUESTIONS)} questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3. BUILD INDEX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_DIR = './data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Create demo data if needed\n",
    "if not os.listdir(DATA_DIR):\n",
    "    with open(f'{DATA_DIR}/rag_overview.txt', 'w') as f:\n",
    "        f.write(\n",
    "            \"HyDE or Hypothetical Document Embeddings improves retrieval by generating a hypothetical answer. \"\n",
    "            \"Multi-query retrieval uses Reciprocal Rank Fusion to deduplicate results from query variants. \"\n",
    "            \"Graph RAG uses knowledge graphs with entity and relation extraction unlike vector RAG. \"\n",
    "            \"Server-Sent Events SSE is used for streaming because it is HTTP-native and one-way. \"\n",
    "            \"Self-query retrieval lets the LLM generate structured metadata filters before vector search.\"\n",
    "        )\n",
    "\n",
    "documents = SimpleDirectoryReader(DATA_DIR).load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(f'Index built from {len(documents)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 4. DEFINE RETRIEVAL STRATEGY RUNNERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.retrievers import QueryFusionRetriever, VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "def run_strategy(strategy_name: str, question: str) -> dict:\n",
    "    \"\"\"\n",
    "    Runs a single question through the specified RAG strategy.\n",
    "    Records answer, retrieved contexts, and latency.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    if strategy_name == 'standard':\n",
    "        engine = index.as_query_engine(similarity_top_k=4)\n",
    "        response = engine.query(question)\n",
    "\n",
    "    elif strategy_name == 'hyde':\n",
    "        base_engine = index.as_query_engine(similarity_top_k=4)\n",
    "        hyde = HyDEQueryTransform(include_original=True)\n",
    "        engine = TransformQueryEngine(base_engine, hyde)\n",
    "        response = engine.query(question)\n",
    "\n",
    "    elif strategy_name == 'multi_query':\n",
    "        retriever = QueryFusionRetriever(\n",
    "            retrievers=[VectorIndexRetriever(index=index, similarity_top_k=4)],\n",
    "            num_queries=4,\n",
    "            mode='reciprocal_rerank',\n",
    "            use_async=False,\n",
    "        )\n",
    "        engine = RetrieverQueryEngine.from_args(retriever)\n",
    "        response = engine.query(question)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unknown strategy: {strategy_name}')\n",
    "\n",
    "    latency = time.time() - start\n",
    "\n",
    "    return {\n",
    "        'strategy': strategy_name,\n",
    "        'question': question,\n",
    "        'answer': str(response),\n",
    "        'contexts': [n.text for n in response.source_nodes] if hasattr(response, 'source_nodes') else [],\n",
    "        'latency_s': round(latency, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "print('Strategy runners defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 5. RUN EVALUATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "STRATEGIES = ['standard', 'hyde', 'multi_query']\n",
    "results = []\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    print(f'\\nEvaluating strategy: {strategy}')\n",
    "    for item in tqdm(EVAL_QUESTIONS):\n",
    "        result = run_strategy(strategy, item['question'])\n",
    "        result['ground_truth'] = item['ground_truth']\n",
    "        results.append(result)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f'\\nâœ… Evaluation complete: {len(df_results)} results')\n",
    "df_results[['strategy', 'question', 'latency_s', 'answer']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 6. RAGAS AUTOMATED SCORING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RAGAS evaluates RAG pipelines using LLM-as-judge methodology.\n",
    "# Reference: https://docs.ragas.io/en/latest/\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,          # Is answer grounded in context? Penalises hallucination.\n",
    "    answer_relevancy,      # Does answer address the question?\n",
    "    context_precision,     # Are retrieved chunks relevant?\n",
    "    context_recall,        # Did we retrieve all needed chunks?\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Build RAGAS-compatible dataset format\n",
    "ragas_data = {\n",
    "    'question':   [r['question']    for r in results],\n",
    "    'answer':     [r['answer']      for r in results],\n",
    "    'contexts':   [r['contexts']    for r in results],\n",
    "    'ground_truth': [r['ground_truth'] for r in results],\n",
    "}\n",
    "ragas_dataset = Dataset.from_dict(ragas_data)\n",
    "\n",
    "# Run RAGAS evaluation â€” this calls the LLM internally to score each sample\n",
    "ragas_scores = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],\n",
    ")\n",
    "\n",
    "df_ragas = ragas_scores.to_pandas()\n",
    "df_ragas['strategy'] = [r['strategy'] for r in results]\n",
    "print('RAGAS evaluation complete!')\n",
    "df_ragas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 7. AGGREGATE SCORES BY STRATEGY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "metric_cols = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "df_grouped = df_ragas.groupby('strategy')[metric_cols].mean().round(3)\n",
    "\n",
    "# Add latency\n",
    "latency_df = df_results.groupby('strategy')['latency_s'].mean().round(2)\n",
    "df_grouped['avg_latency_s'] = latency_df\n",
    "\n",
    "print('\\nðŸ“Š Strategy Comparison:')\n",
    "display(df_grouped.style.background_gradient(cmap='RdYlGn', axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 8. VISUALISATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Advanced RAG Strategy Comparison â€” Day 15', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: Metric heatmap\n",
    "sns.heatmap(\n",
    "    df_grouped[metric_cols],\n",
    "    ax=axes[0],\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    linewidths=0.5,\n",
    ")\n",
    "axes[0].set_title('RAGAS Metrics by Strategy')\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Plot 2: Latency vs. quality radar (simplified bar)\n",
    "strategies = df_grouped.index.tolist()\n",
    "overall_quality = df_grouped[metric_cols].mean(axis=1)\n",
    "latencies = df_grouped['avg_latency_s']\n",
    "\n",
    "x = range(len(strategies))\n",
    "bars1 = axes[1].bar([i - 0.2 for i in x], overall_quality, 0.35,\n",
    "                     label='Avg Quality Score', color='#4EA8DE', alpha=0.85)\n",
    "ax2 = axes[1].twinx()\n",
    "bars2 = ax2.bar([i + 0.2 for i in x], latencies, 0.35,\n",
    "                 label='Avg Latency (s)', color='#F6AD55', alpha=0.85)\n",
    "\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(strategies)\n",
    "axes[1].set_ylabel('Quality Score (0-1)', color='#4EA8DE')\n",
    "ax2.set_ylabel('Latency (seconds)', color='#F6AD55')\n",
    "axes[1].set_title('Quality vs. Latency Trade-off')\n",
    "axes[1].legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_evaluation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('\\nâœ… Chart saved to rag_evaluation_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 9. CUSTOM HALLUCINATION CHECK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# A simple LLM-as-judge prompt for detecting hallucinations.\n",
    "# More interpretable than black-box RAGAS scores for demo/interview purposes.\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "HALLUCINATION_PROMPT = \"\"\"\n",
    "You are a fact-checker. Given a context and an answer, determine if the answer \n",
    "contains any claims NOT supported by the context (hallucinations).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Respond with JSON only:\n",
    "{{\"hallucination_detected\": true/false, \"explanation\": \"brief reason\", \"confidence\": 0.0-1.0}}\n",
    "\"\"\"\n",
    "\n",
    "def check_hallucination(context: str, answer: str) -> dict:\n",
    "    import json, re\n",
    "    prompt = HALLUCINATION_PROMPT.format(context=context[:1500], answer=answer[:500])\n",
    "    raw = llm.complete(prompt).text.strip()\n",
    "    raw = re.sub(r'```(?:json)?|```', '', raw).strip()\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except:\n",
    "        return {'hallucination_detected': None, 'explanation': 'parse_error', 'confidence': 0}\n",
    "\n",
    "\n",
    "# Run hallucination checks on first question for each strategy\n",
    "print('\\nðŸ” Hallucination Check Results:')\n",
    "for strategy in STRATEGIES:\n",
    "    row = df_results[(df_results.strategy == strategy)].iloc[0]\n",
    "    ctx = ' '.join(row['contexts'])[:1000]\n",
    "    result = check_hallucination(ctx, row['answer'])\n",
    "    status = 'ðŸŸ¢ Clean' if not result.get('hallucination_detected') else 'ðŸ”´ HALLUCINATION'\n",
    "    print(f'  [{strategy:12s}] {status} | Confidence: {result.get(\"confidence\",0):.2f} | {result.get(\"explanation\",\"\")[:80]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 10. INTERVIEW SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('='*65)\n",
    "print('ðŸ“‹  INTERVIEW-READY SUMMARY: WHEN TO USE EACH STRATEGY')\n",
    "print('='*65)\n",
    "\n",
    "summary = {\n",
    "    'Standard RAG':  'Best for simple factual queries, lowest latency, baseline',\n",
    "    'HyDE':          'Best for zero-shot domains, improves recall by 15-30%, adds ~0.5s latency',\n",
    "    'Multi-Query':   'Best for ambiguous queries, highest recall, adds 2-4x LLM calls',\n",
    "    'Graph RAG':     'Best for multi-hop relational queries, requires graph construction overhead',\n",
    "    'Self-Query':    'Best when metadata filtering is needed (e.g., date, author, category)',\n",
    "}\n",
    "\n",
    "for strategy, use_case in summary.items():\n",
    "    print(f'  {strategy:15s} â†’ {use_case}')\n",
    "\n",
    "print('\\nðŸ“Š Key Metric Targets:')\n",
    "print('  Faithfulness > 0.85 (hallucination guard)')\n",
    "print('  Context Recall > 0.70 (retrieval completeness)')\n",
    "print('  TTFT < 500ms (streaming UX threshold)')\n",
    "print('\\nâœ… Day 15 evaluation complete!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
