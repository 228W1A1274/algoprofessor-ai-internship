Regression and classification both are supervised models 
For both we sue label data
Both are used to solve machine learning algorithms
Regression-continuous value(correlation between dependent variables and independent variables example house prices)
classification-discrete value(email spam or not spam)

Regression is used to divide into classes based on parameters
Linear regression:
It is statical method for continuous predictions
Simple linear regression: Single independent variable is used to predict one dependent variable(e.g., salary from hours studied))
multiple linear regression:  independent variable is used to predict one dependent variable(e.g., salary from hours studied, age, and sleep)

Logistic regression:

To convert continuous values in range of 0 to 1 by using sigmoid function
We use decision boundary as threshold. Here we sue Decision boundary as linear.



Feature engineering:
(An outlier in a dataset is a data point that significantly differs from other observations, appearing as an unusually high or low value that doesn't fit the general pattern, often due to measurement errors, data entry mistakes, or representing a truly rare event, and can skew analysis, requiring careful identification and handling.
)

Means taking the raw data and extract meaningful information that ml model can understand

->It is a process of selecting and modifying features form the dataset while creating a predictive model using machine learning

Different techniques in future engineering:
1)handling missing data(like NaN,null,none,-.?)It is done by removing data and replacing data with mean median or mode.

2)Handling continuous values(we use normalization and standardization)
.Normalization: Normalizing also knows as min-max scaling is used to transform features into a similar scale(it scales values between 0 and 1)
   .Useful when data in dataset or on different scales
   .Scikit-learn library provides the Minmaxscaler
   .when data as no outliers
.Standardization: It is a scaling technique  is a machine learning preprocessing technique that transforms numerical features to have a mean of 0 and a standard deviation of 1.

3)Handling Categorical values: It is a data that can be represents categories like country gender.(They need to converted into numerical values for ml model)
.For conversion e sue label encoding and one-hot-encoding.
.Label encoding converts a categorical values of a column into number (here each label is given a unique integer based on alphabetical order)(it is present in scikit-learn)
.one-hot-encoding adds new features based on unique values (it creates dummy variables where each category is resented as one-hot vector it is also present in scikit learn)

Features election:It is process of sleceting the features either manually or automatically which contribute the most if the prediction value
  ->Reduce the complexity of ml model
  ->Improve the accuracy of model
  ->Used to reduce overfitting
  ->To train faster

Methods for feature selection:
1)removing features with low variance:(A certain threshold is selected for variance by using p(1-p) formula and remove all the features whose variance does not meet the threshold zero-variance features are also removed)

2)univariate feature selection:(selecting independent features that ahs major impact on target feature selectkbest class is sued to select the features  here k represent top most features) 

3)Recursive feature elimination:(Important features are identified by coef, feature_imporatnces attributes)

4)Fs using selectfrommodel: It is based on specific  attribute such as coef_ or feature_imporatnces threshold defaultly mean is taking as threshold

5)Sequential future selection:(By moving forward or backward bases on cross validation selector ion estimator)

Correlation matrix heatmap: It shows relationship between the features or target features

Pca-used for dimensionality reduction(reduce number of features)
Find a new set of orthogonal(perpendicular) feature vectors in such a way that data spread is maximum in the direction of feature vector.


We will consider the feature which has more variance(more data spread) so  we will rotate the axis with respect to that future then we will project the all features on to that axis

Steps include  in this
->Standardize the datasets by Xnew=X-mean(X)/std(X)(That means  we calculate mean std for every dataset
->Compute covariance matrix for dataset
->Eigenvalues eigen vectors of that covariance matrix(here Eigen vectors are orthogonal)(Data where eigenvalue is more then data spread  is more)
->Sort the eigenvectors by decreasing eigen values
->Chose a eigen vectors with largest eigenvalues to form d*k dimensional matrix w
->Transform the samples onto new subspace

T-SNE:
->we start with original scatter plot then we put the points on the number line in random order
->from tsne moves the points as time passes until points are clustered(should it move left or right)
->At each point on the line or axis it checks for the points to attract  repel and then they go closer to their original cluster points and forms clusters on a single line

1)Determine the similarity of all points in scatterplot(By calculating unscaled similarity between all the points)
2)The next step is to scale the similarities to add up to 1(Why to 1?Tnse has a perplexity parameter equal to the expected density around each point and that comes into play but these clusters are still more similar )
2)The averages the similarity scores of two different directions then e get similarity score of matrix


BOOSTING WORKS ON DECISION TREE:

Gradient boosting:

->In gradient boosting we model the Function by taking the constant value which can be considered as  some kind of basis ad then add multiple weighted weak learners to it whose role is to bring the prediction to our labels(This means we take initial guess and then we calculate error or residual then we will add weak learners(means small changes  to solve these residuals) and get closer to labels by minimizing the loss

1)computer the intial prediction f0(a guess in regression it is average of the labels)
2)Compute the residuals(or error between our prediction and labels)
3)train a tree to predict the residuals
4)computer the new best approximation

XGBOOST:(gpu)

->XGBoost is extreme gradient boosting it is machine learning algorithm mainly on tabular datasets
->Eg boosting is extension of gradient boosting it has capabilities of

  .Regularization Capability: XGBoost includes built-in L1 (Lasso) and L2 (Ridge) regularization, which effectively penalizes model complexity to reduce overfitting.

  .Missing Value Handling: The algorithm features built-in capabilities to automatically learn the best direction for missing values, removing the need for manual data imputation.

  .In-built Cross-Validation: It supports integrated cross-validation and early stopping, which halts training when no further improvement is detected, ensuring the optimal number of iterations.
  
  .Speed and Scalability: Designed for high performance, it utilizes parallel processing and distributed computing frameworks, making it ideal for handling very large datasets.

   .Depth-First Tree Growth: It employs a depth-first search strategy and utilizes a "max_depth" approach followed by post-pruning, allowing trees to grow deeper and more optimized compared to "greedy" pruning methods.

   .Customizable Optimization: It supports user-defined objective functions and evaluation metrics, allowing for specialized optimizations tailored to specific business or research needs.
    
   .Versatile Interfaces: It provides an easy-to-use interface across multiple languages, including Python, R, Java, and C++. 


LIGHTGBM:(local machine)

->HISTOGRAM:WHERE WE WILL MAKE BINS OR ORIGINAL VALUE STO MAKE TARINING FASTER EXAMPLE LET US CONSIDER A COLUMN AS VALUES OF 50 55 60 THENW E WILL MAKE IT AS 50-60 BIN

->EXCLUSIVE FEATURE BUNDLING: Exclusive features that cannot be same at the same time for suppose there is column of gender sometimes male as 1 and sometimes female as 1 no chance both becoming 1 light gbm makes them bundling means when it sees 10 it keeps as 11 when its sees 01 it keeps as 10 for reducing dimensionality

->Gradient based one side sampling:
It takes all the gradients of samples and sort them in descending order and then it will take 20 percent of highest gradients s it is and it will take 10 percent randomly sleeted(Samples) data in below 80 percent and combine them so it makes only one side sampling to reduce the training .



->Seaborn is a powerful Python data visualization library based on Matplotlib that creates attractive, informative statistical graphics with minimal code. It integrates seamlessly with Pandas data structures, making it ideal for exploring relationships, distributions, and patterns in data. Key uses include generating complex plots like heatmaps, violin plots, and regression lines easily. 

->In Seaborn, the sns.load_dataset() function is a specialized utility used to fetch built-in example datasets directly into your environment as Pandas DataFrames. 


->Hyperparameters are the parameters that are not trained during the training they are set by the suers before the training .They are the configuration settings that control the learning process and model behavior(eg:max depth in tree,no of hidden layers)
->They balance the tradeoff between overfitting and underfitting
->Hyperparameters depend on dataset
->Grid search is an automated, exhaustive tuning technique in machine learning that finds the optimal hyperparameters for a model by systematically scanning through a user-specified subset of all possible parameter combinations(it is computationally expensive and used for small or less hyperparameters)
->Randomized search -Users defined range for each hyperparameter and specify how many models i am thinking to train.(it is used for large datasets and many hyperparameters)


NEURAL NETWORKS:
->Neural networks are composed of node layers(input layer,hidden layer,output layer).Where each node is a artificial neuron
->use a linear regression (the weights of the connections between the nodes determines how much influence each input has on the output)
->Data is passes between nodes(feed forward neural network)
->Nn relies on training data to learn and improve their accuracy
->Neural networks are of different types

Multilayer preceptron:


->The perceptron is inspired by neuron in our brain
->Perceptrons are organized in layers(known as multilayer perceptron)

Activation function:
1)sigmoid function:2/1+e-x(0,1)(binary classification)
2)relu:(max,0)(hidden layers neural networks mainly cnn to solve vanishing gradient problem)
3)SoftMax:multi class problems

Lossfunction:

->Loss means it  finds the degree of error of ai model (It finds by comparing the predicted value with actual value)

->loss function(regression and classification)
Regression loss functions:
1)Mean Squared Error: Average of squared difference between the predicted value to actual value ,it is more sensitive to outliers
2)Mean absolute error: Measures  the mean absolute difference between the predict value to actual value. It is less sensitive to outliers.
3)Huber loss: Its a comprehensive for mse and mae ,it behaves like mse for small errors and mae for large errors

Classification lass functions:(accuracy, it measures how well the model predicts the outcome rather than continuous values)
1)cross entropy loss function: Its widely used loss function for classification tasks(where entropy is the measure of uncertainty within a system(entropy is high when possible outcomes are high)(cross entropy loss measures how uncertain the models predictions are compared to actual outcomes)

2)Hinge cross entropy: This is commonly used for vector machines it measures the level of confidence(it is used for binary classification tasks)

-
>Loss function indicates the numeric value that shows how far off the models predictions are from the actual results,by analyzing the loss we can adjust models parameters typically through a process called optimization

->It can also acts as input for an algorithm that actually influence model parameters to minimize the loss (eg gradient over loss function)


Optimizers:

->Defines values of parameters such that loss function is at its lowest(they dont know loss):
1)gradient descent:The weight is updated only once after one taring loop(stochastic update after every weight ,mini-batch gradient descent)
2)adptive(adadelta)(learning rate)
3)adam(gradient escent with momentum)It computes individual, adaptive learning rates for different parameters by combining the benefits of Momentum (using moving averages of past gradients) and RMSprop (scaling updates by the magnitude of recent squared gradients). 


TensorFlow:
It is a opensource deep learning framework for powering deep neural networks high level code.It is firstd evevloepd by google in 2015 It is commonly sued inpython and it can also sued in c++,javascript
It is a library with linear algebra and statistics that  simplifies the process of building, training, and deploying AI models across various platforms. 



TensorFlow's architecture is based on the concept of data flow graphs, where: 
->Tensors are multi-dimensional arrays (vectors or matrices) that represent data flowing through the system.

->Nodes in the graph represent mathematical operations (e.g., addition, multiplication, division) that are performed on the tensors.

->The framework, primarily using Python as a front-end API, manages how data moves through these operations, with the actual computations performed efficiently by high-performance C++ binaries. web scrapping


keras:
-Keras is a high-level, user-friendly deep learning API written in Python, designed to enable fast experimentation with neural networks by providing simple, consistent interfaces.

You need to move from idea to result quickly without getting bogged down in low-level mathematical operations or complex syntax.

You require a balance between ease of use and flexibility, as it provides high-level APIs but also allows for lower-level control and customization through subclassing if needed.


Transfer learning:

Transfer learning is using a model that has been trained on other data or large dataset and using it for small dataset or other data(This is know as fine-tuning of pretrained data)



NLP:
->EXAMPLE Considered is Camtasia 
->Camtasia is a video recording software they have this support system where  when people find any issues they report in website and get a ticket.
->To identify whether ticket is high medium or low priority we use the nlp model by looking up the text in ticket saying its priority
NLP Has several steps:

1)Data Acquisition:
It is the process of collecting data from the team or source (like collecting data from database or collecting data from cloud or use public dataset ) required to solve a NLP problem.

Some of the ways are

1)Use public dataset(web scrapping(Web scraping is an automated method using bots to extract large amounts of data—such as text, images, and prices—from websites, converting unstructured HTML into structured formats like CSV or JSON. It involves sending HTTP requests to a site, parsing the DOM structure, and storing the targeted information. )

2)Product intervention(product intervention refers to the intentional development of instrumentation within a software product, application, or service to directly capture high-quality, relevant data for training machine learning models.)

3)Data augmentation

2)Text Extraction and cleanup:
->after collecting data we can delete irrelevant data  and make sure important or releevant information is available
->Correcting spelling mistakes
->Removing extra line breaks or something 

3)Preprocessing 

1)Sentence segmentation and tokenization

->We need to split the entire dataset or text into smaller subsets of data or segments(Looking dots or question marks or something depending upon rules and grammar of particular language and segmenting data )(Libraries like NLTK AND spaCy has this kind of ready tokenizer)


2)Word tokenization

->After creating separate sentences next process separating that sentences into words that words are used for training model

3)Stemming(It does not know grammar)

->Stemming in NLP is a text normalization technique that reduces inflected or derived words to their root/base form (the "stem") by crudely removing suffixes and prefixes, such as reducing "flying" and "flied" to "fli".

4)lemmatization

->Lemmatization in NLP is a text normalization technique that reduces inflected or derived words to their base dictionary form, known as a lemma (e.g., "running" to "run", "better" to "good"). It utilizes vocabulary, morphological analysis, and part-of-speech (POS) tagging to ensure accuracy, making it more precise but slower than stemming. 

4)Feature engineering

->machine learning models cannot understand textual data they need numbers so we convert text into numbers by using some different techniques(where simmialr words ahs similar number  representation)

Some techniques:
1)TF-IDF VECTORIZER:
TF-IDF Vectorizer in NLP transforms text into numerical vectors based on word importance, calculated by multiplying Term Frequency (TF) and Inverse Document Frequency (IDF). 



2)ONE HOT ENCODDING

3)WORD EMBEDDING

Word embeddings in NLP represent words as dense, semantic vectors in a continuous space, placing similar words closer together (e.g., "king" and "queen" are closer than "king" and "apple"). Popular models like Word2Vec or Glove convert words into numerical vectors, allowing machines to perform arithmetic on them, such as \(king-man+woman=queen\)


5)To build Ml model(Sometimes it requires ml model in nlp sometimes nlp dosnet require them)

6)Deployment(Write fastapior flask service and deploy taht in cloud)

7)Monitor and update



TF-IDF:
->If some word is coming in so many documents it may be common term it does not influence at higher extent so we can give less scoring  (Inverse document frequency formula) if some word is coming in less documents or data then it may be important for that vector so we need to give high frequency this can be done through

Document frequency=Number of times term t is present in all docs

Inverse document frequency formula(t)=log(Total documents/Number of documents term t is present in it)

->Term frequency  TF(t,d)=(Total Number of time t in present in doc a)/(Total number of tokens in doc a)

TF-IDFE=Tf(t,d)*IDF(t)

->Why log is used in idf :Even through how many times a word occurs in each document sometimes at higher extent we can say that word is important and word is not important (o dampen the effect of very high-frequency words and to manage the vast range of possible IDF values, preventing rare words from disproportionately dominating the final TF-IDF score. )

Limitations:
->It does not  capture the relationship between the words
->AS increased dimensionality sparsity increase(means most of columns contain zeros,NAN,None)

->Out of vocabulary program(tfidfvectorizer is present in sklearn)get_feature_names_out() provides all features in order



The Memory (RNNs & LSTMs):

RNN:(speech recognition, Text recognition,)

Rnn are good at modelling sequential data for predictions

In feed forward neural networks information is passed between layers and information form previous layers affect the latter ones 
If we had a loop in feed forward neural network which pass previous information forward then it is rnn

Short term memory is the issue in rnn(This is due to vanishing gradient desent):It is caused by infamous vanishing gradient problem(That means if tehre are more hidden states it is difficult for rnn to retain apst information at every hidden state)


To mitigate short term memory e use two specialized rnns like lstm and gru(gated reccurnet units):They are capable of learning long term  memory



Transfer models bert:

->Transformers are based on 2017 paper Attention is all you need

->Before transformers all models can convert words to vectors(These vectors dosent provide the context and usage of context when words changes based on context)

->Transformer is a encoder decoder model that uses attention mechanism(It can take the advantage of penalization and process much more data in the same amount of data)

->Transformers are built on attention mechanism as core

->Transformer contains encoder and decoder

->Encoder encodes the input sequence and passes it to the decoder and decoder decodes tehr representation of a relevant task

->Encoder is composed with sequence of encoders in it(Six encoders on top of each other its just  a hyperparameter)

->Where each encoder is divided into wo sublayers
   .The first layer is called self-attention(Where input is taken into self attention first
 which encodes or look central word in the input sequence (it identifies important words)(The each word of sentence is passed through self attention layer  then it passes through feed forward layer)
   .The second layer is feed-forward layer (The output of self attention layer is passed into feed forward this feed forward network is applied on each individual position separately)

->Where each decoder is divided into three sublayers
  .Self-attention
  .Encoder-decoder attention-That helps the decoder to focus on relevant parts of input
  .Feedforward

->In self attention layer the input  embedding in broken into query key and value vectors
These vectors are computed with weights that transformer learns during the training process

->After value vector it is multiplied by SoftMax and then sum them up

->We have to sum up weighted  value vectors which produce the output of self attention layer at this position 

->For the first word you can send the resulting vector to feed forward neural betwork


The total process:
1)Input the natural language sentences
2)Embed each word
3)Perform multi-headed attention and multiply embedded words with the respective weight matrices
4)Calculating the attention using QKV matrices
5)Concatenate the matrices to produce the output matrix which is the same dimension as the final matrix


Pretarined  transformer models:
1)BART(Uses both encoder and decoder)
2)BERT(Uses encoder only)
3)GPS GP2 (Decoder only)


bert:(bidirectional encoder representations from transformers) developed by google in 2018

BERT is trained in two variations:

->One model is Bert bas which had 12 stack of transformers with approximately 110 million parameters) 
->second Bert large 24 layers of transformers with about 340 million parameters

->Able to handle long input context

->Trained on entire Wikipedia and book corpus

->It ahs multi task objective

->Trained on tpu
->It works on sentence level and token level

BERT WORKING:

(masking is a technique used in the attention mechanism to prevent the model from "cheating" by looking at future tokens during training, which is crucial for autoregressive tasks like text generation. It ensures that when predicting the next token, the model only attends to previous tokens in the sequence. )


->It is trained on two different models

1)masked language model(Where sentence are masked and the model is trained to predict the masked words)(If you want to train Bert from scratch you want to convert vitrain percentage of the words in your corpus)(Recomended amsking 15)


2)Next sentence prediction:
For example the model is given two sets of sentences (It learns the relationship between sentences and predict the next sentence given the first in)(This is a binary classification tsk)



Three different embeddings in BERT:
1)Token embedding 
2)Segment embedding
3)Position embedding

Bert is used for text classification(sentence-pair classification, Question answering, Single-sentence tagging tasks)






->









