{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiIpncm8Hb01",
        "outputId": "576b2822-1f1c-4c69-841d-574bc841d231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment Setup Complete.\n"
          ]
        }
      ],
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install nltk transformers scikit-learn tensorflow pandas numpy\n",
        "\n",
        "# 2. Download NLTK data (stopwords, wordnet)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 3. Create folders for organization\n",
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "print(\"Environment Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "nltk\n",
        "pandas\n",
        "numpy\n",
        "scikit-learn\n",
        "tensorflow\n",
        "transformers\n",
        "joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHe7XYtTIMVI",
        "outputId": "9193a79e-0cc1-4841-9777-02823129f063"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_preprocessing.py\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Initialize the tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Takes a raw string and cleans it:\n",
        "    1. Lowercase\n",
        "    2. Remove punctuation\n",
        "    3. Remove stopwords\n",
        "    4. Lemmatize (run -> running)\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove Punctuation (replace with space)\n",
        "    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
        "\n",
        "    # 3. Tokenize (Split into words)\n",
        "    tokens = text.split()\n",
        "\n",
        "    # 4. Remove Stopwords & Lemmatize\n",
        "    clean_tokens = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in tokens\n",
        "        if word not in stop_words and len(word) > 2\n",
        "    ]\n",
        "\n",
        "    return \" \".join(clean_tokens)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test it\n",
        "    sample = \"The movie was absolutely amazing! I loved the acting and the plot was running fast.\"\n",
        "    cleaned = clean_text(sample)\n",
        "    print(f\"Original: {sample}\")\n",
        "    print(f\"Cleaned:  {cleaned}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jVnhr6zJq3C",
        "outputId": "ea995a0d-d0dd-44b7-a6c9-b1ca0e63ed0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing text_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python text_preprocessing.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtNGJbakJrsS",
        "outputId": "6ce106a9-469f-42b0-ebcd-ebe967e54a4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The movie was absolutely amazing! I loved the acting and the plot was running fast.\n",
            "Cleaned:  movie absolutely amazing loved acting plot running fast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sentiment_analyzer.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "from text_preprocessing import clean_text\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "def train_classic_sentiment():\n",
        "    print(\"--- 1. Loading FULL IMDB Dataset ---\")\n",
        "\n",
        "    # Step 1: Download and Extract (Keras handles this automatically)\n",
        "    dataset_path = tf.keras.utils.get_file(\n",
        "        fname=\"aclImdb_v1.tar.gz\",\n",
        "        origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "        extract=True\n",
        "    )\n",
        "\n",
        "    # Step 2: Find the folder\n",
        "    # Keras usually downloads to ~/.keras/datasets/\n",
        "    # The extracted folder is named 'aclImdb'\n",
        "    base_dir = os.path.dirname(dataset_path)\n",
        "    data_dir = os.path.join(base_dir, 'aclImdb', 'train')\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(data_dir):\n",
        "        # Fallback: Sometimes Keras returns the directory path itself\n",
        "        data_dir = os.path.join(dataset_path, 'aclImdb', 'train')\n",
        "\n",
        "    print(f\"Data directory located at: {data_dir}\")\n",
        "\n",
        "    # Step 3: Load Data\n",
        "    print(\"Loading data files (This takes a moment)...\")\n",
        "    try:\n",
        "        data = load_files(data_dir, categories=['pos', 'neg'], encoding='utf-8', decode_error='replace')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: Could not find 'pos' and 'neg' folders inside {data_dir}\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
        "    print(f\"Dataset Shape: {df.shape} (Full Training Data)\")\n",
        "\n",
        "    print(\"--- 2. Preprocessing (Cleaning 25,000 reviews...) ---\")\n",
        "    df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "    X = df['clean_text']\n",
        "    y = df['label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"--- 3. Vectorization (TF-IDF) ---\")\n",
        "    vectorizer = TfidfVectorizer(max_features=10000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    print(\"--- 4. Training Logistic Regression ---\")\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    preds = model.predict(X_test_vec)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"\\nClassic Model Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    joblib.dump(model, 'models/classic_sentiment_model.pkl')\n",
        "    joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')\n",
        "    print(\"Model saved to models/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_classic_sentiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBexqDMyJxiL",
        "outputId": "c8b8f13a-cf59-4399-b4ac-c7fd1a4e2e54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sentiment_analyzer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sentiment_analyzer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfRER32LKyAx",
        "outputId": "0860ae84-185c-4c8e-f42e-3cd9a0f65e09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-05 10:07:07.334361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770286027.369109     905 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770286027.378967     905 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770286027.407180     905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770286027.407222     905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770286027.407231     905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770286027.407238     905 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-05 10:07:07.414064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "--- 1. Loading FULL IMDB Dataset ---\n",
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Data directory located at: /root/.keras/datasets/aclImdb_v1_extracted/aclImdb/train\n",
            "Loading data files (This takes a moment)...\n",
            "Dataset Shape: (25000, 2) (Full Training Data)\n",
            "--- 2. Preprocessing (Cleaning 25,000 reviews...) ---\n",
            "--- 3. Vectorization (TF-IDF) ---\n",
            "--- 4. Training Logistic Regression ---\n",
            "\n",
            "Classic Model Accuracy: 88.08%\n",
            "Model saved to models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Uninstall the conflicting libraries\n",
        "!pip uninstall -y tensorflow transformers ml_dtypes jax jaxlib\n",
        "\n",
        "# 2. Install PyTorch and Transformers (The stable combination)\n",
        "!pip install torch transformers scikit-learn pandas accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGtqZErkO7xQ",
        "outputId": "b645918d-2967-48f0-c25d-244b2625505f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Found existing installation: transformers 5.0.0\n",
            "Uninstalling transformers-5.0.0:\n",
            "  Successfully uninstalled transformers-5.0.0\n",
            "Found existing installation: ml_dtypes 0.5.4\n",
            "Uninstalling ml_dtypes-0.5.4:\n",
            "  Successfully uninstalled ml_dtypes-0.5.4\n",
            "Found existing installation: jax 0.7.2\n",
            "Uninstalling jax-0.7.2:\n",
            "  Successfully uninstalled jax-0.7.2\n",
            "Found existing installation: jaxlib 0.7.2\n",
            "Uninstalling jaxlib-0.7.2:\n",
            "  Successfully uninstalled jaxlib-0.7.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting transformers\n",
            "  Downloading transformers-5.0.0-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Downloading transformers-5.0.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "Successfully installed transformers-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile transformer_model.py\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_files\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cpu':\n",
        "    print(\"WARNING: You are training BERT on a CPU. This will be very slow (Hours).\")\n",
        "\n",
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "\n",
        "def train_bert_model():\n",
        "    print(\"--- 1. Loading FULL Data (25,000 Reviews) ---\")\n",
        "\n",
        "    if not os.path.exists(\"aclImdb/train\"):\n",
        "        print(\"Downloading dataset...\")\n",
        "        url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "        if not os.path.exists(\"aclImdb_v1.tar.gz\"):\n",
        "            os.system(f\"wget {url}\")\n",
        "        os.system(\"tar -xzf aclImdb_v1.tar.gz\")\n",
        "\n",
        "    data_dir = \"aclImdb/train\"\n",
        "    data = load_files(data_dir, categories=['pos', 'neg'], encoding='utf-8', decode_error='replace')\n",
        "    texts = data.data\n",
        "    labels = data.target\n",
        "\n",
        "    # --- REMOVED THE LIMIT HERE ---\n",
        "    # Now using all 25,000 samples\n",
        "    print(f\"Training on {len(texts)} samples.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"--- 2. Tokenization ---\")\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)\n",
        "    test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    train_dataset = IMDBDataset(train_encodings, y_train)\n",
        "    test_dataset = IMDBDataset(test_encodings, y_test)\n",
        "\n",
        "    print(\"--- 3. Training with DistilBERT ---\")\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "    model.to(device)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=100,               # Log less frequently\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"--- 4. Saving Model ---\")\n",
        "    model.save_pretrained(\"./models/bert_sentiment\")\n",
        "    tokenizer.save_pretrained(\"./models/bert_sentiment\")\n",
        "    print(\"Model saved to ./models/bert_sentiment\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_bert_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2wnxJn_QCJv",
        "outputId": "4bcf8249-ca2d-42fd-815e-1e92a208962f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing transformer_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python transformer_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsdFABLoQGdq",
        "outputId": "b0d36735-e59d-4cf0-f124-9bb7a90b4a1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "--- 1. Loading FULL Data (25,000 Reviews) ---\n",
            "Downloading dataset...\n",
            "--2026-02-05 10:10:41--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  38.4MB/s    in 2.1s    \n",
            "\n",
            "2026-02-05 10:10:43 (38.4 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Training on 25000 samples.\n",
            "--- 2. Tokenization ---\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 185kB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 2.10MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.87MB/s]\n",
            "--- 3. Training with DistilBERT ---\n",
            "config.json: 100% 483/483 [00:00<00:00, 2.32MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:03<00:00, 71.3MB/s]\n",
            "Loading weights: 100% 100/100 [00:00<00:00, 1412.95it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]\n",
            "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-uncased\n",
            "Key                     | Status     | \n",
            "------------------------+------------+-\n",
            "vocab_transform.bias    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "vocab_projector.bias    | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "vocab_transform.weight  | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "vocab_layer_norm.bias   | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "vocab_layer_norm.weight | \u001b[38;5;208mUNEXPECTED\u001b[0m | \n",
            "pre_classifier.weight   | \u001b[31mMISSING\u001b[0m    | \n",
            "pre_classifier.bias     | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.bias         | \u001b[31mMISSING\u001b[0m    | \n",
            "classifier.weight       | \u001b[31mMISSING\u001b[0m    | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "- \u001b[31mMISSING\u001b[0m\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
            "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n",
            "{'loss': '0.5912', 'grad_norm': '3.357', 'learning_rate': '4.934e-05', 'epoch': '0.04'}\n",
            "{'loss': '0.476', 'grad_norm': '14.87', 'learning_rate': '4.867e-05', 'epoch': '0.08'}\n",
            "{'loss': '0.4629', 'grad_norm': '7.397', 'learning_rate': '4.801e-05', 'epoch': '0.12'}\n",
            "{'loss': '0.4386', 'grad_norm': '5.245', 'learning_rate': '4.734e-05', 'epoch': '0.16'}\n",
            "{'loss': '0.4098', 'grad_norm': '7.181', 'learning_rate': '4.667e-05', 'epoch': '0.2'}\n",
            "{'loss': '0.4422', 'grad_norm': '2.389', 'learning_rate': '4.601e-05', 'epoch': '0.24'}\n",
            "{'loss': '0.4054', 'grad_norm': '7.115', 'learning_rate': '4.534e-05', 'epoch': '0.28'}\n",
            "{'loss': '0.4412', 'grad_norm': '16.32', 'learning_rate': '4.467e-05', 'epoch': '0.32'}\n",
            "{'loss': '0.388', 'grad_norm': '0.5331', 'learning_rate': '4.401e-05', 'epoch': '0.36'}\n",
            "{'loss': '0.408', 'grad_norm': '21.46', 'learning_rate': '4.334e-05', 'epoch': '0.4'}\n",
            "{'loss': '0.4375', 'grad_norm': '9.587', 'learning_rate': '4.267e-05', 'epoch': '0.44'}\n",
            "{'loss': '0.352', 'grad_norm': '26.84', 'learning_rate': '4.201e-05', 'epoch': '0.48'}\n",
            "{'loss': '0.3963', 'grad_norm': '10.82', 'learning_rate': '4.134e-05', 'epoch': '0.52'}\n",
            "{'loss': '0.3419', 'grad_norm': '8.615', 'learning_rate': '4.067e-05', 'epoch': '0.56'}\n",
            "{'loss': '0.3771', 'grad_norm': '5.981', 'learning_rate': '4.001e-05', 'epoch': '0.6'}\n",
            "{'loss': '0.392', 'grad_norm': '4.556', 'learning_rate': '3.934e-05', 'epoch': '0.64'}\n",
            "{'loss': '0.3512', 'grad_norm': '5.918', 'learning_rate': '3.867e-05', 'epoch': '0.68'}\n",
            "{'loss': '0.355', 'grad_norm': '12.84', 'learning_rate': '3.801e-05', 'epoch': '0.72'}\n",
            "{'loss': '0.3834', 'grad_norm': '2.214', 'learning_rate': '3.734e-05', 'epoch': '0.76'}\n",
            "{'loss': '0.3507', 'grad_norm': '2.626', 'learning_rate': '3.667e-05', 'epoch': '0.8'}\n",
            "{'loss': '0.3894', 'grad_norm': '16.08', 'learning_rate': '3.601e-05', 'epoch': '0.84'}\n",
            "{'loss': '0.358', 'grad_norm': '5.887', 'learning_rate': '3.534e-05', 'epoch': '0.88'}\n",
            "{'loss': '0.3586', 'grad_norm': '10.47', 'learning_rate': '3.467e-05', 'epoch': '0.92'}\n",
            "{'loss': '0.3702', 'grad_norm': '7.47', 'learning_rate': '3.401e-05', 'epoch': '0.96'}\n",
            "{'loss': '0.3525', 'grad_norm': '15.93', 'learning_rate': '3.334e-05', 'epoch': '1'}\n",
            " 33% 2500/7500 [04:04<08:13, 10.12it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 3/313 [00:00<00:11, 26.46it/s]\u001b[A\n",
            "  2% 6/313 [00:00<00:14, 20.59it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:15, 19.02it/s]\u001b[A\n",
            "  4% 11/313 [00:00<00:16, 18.80it/s]\u001b[A\n",
            "  4% 13/313 [00:00<00:16, 18.45it/s]\u001b[A\n",
            "  5% 15/313 [00:00<00:16, 18.08it/s]\u001b[A\n",
            "  5% 17/313 [00:00<00:16, 17.74it/s]\u001b[A\n",
            "  6% 19/313 [00:01<00:16, 17.74it/s]\u001b[A\n",
            "  7% 21/313 [00:01<00:16, 17.77it/s]\u001b[A\n",
            "  7% 23/313 [00:01<00:16, 17.75it/s]\u001b[A\n",
            "  8% 25/313 [00:01<00:16, 17.74it/s]\u001b[A\n",
            "  9% 27/313 [00:01<00:16, 17.68it/s]\u001b[A\n",
            "  9% 29/313 [00:01<00:16, 17.73it/s]\u001b[A\n",
            " 10% 31/313 [00:01<00:15, 17.68it/s]\u001b[A\n",
            " 11% 33/313 [00:01<00:15, 17.59it/s]\u001b[A\n",
            " 11% 35/313 [00:01<00:15, 17.53it/s]\u001b[A\n",
            " 12% 37/313 [00:02<00:15, 17.56it/s]\u001b[A\n",
            " 12% 39/313 [00:02<00:15, 17.54it/s]\u001b[A\n",
            " 13% 41/313 [00:02<00:15, 17.77it/s]\u001b[A\n",
            " 14% 43/313 [00:02<00:15, 17.70it/s]\u001b[A\n",
            " 14% 45/313 [00:02<00:15, 17.77it/s]\u001b[A\n",
            " 15% 47/313 [00:02<00:15, 17.70it/s]\u001b[A\n",
            " 16% 49/313 [00:02<00:15, 17.52it/s]\u001b[A\n",
            " 16% 51/313 [00:02<00:14, 17.62it/s]\u001b[A\n",
            " 17% 53/313 [00:02<00:14, 17.55it/s]\u001b[A\n",
            " 18% 55/313 [00:03<00:14, 17.68it/s]\u001b[A\n",
            " 18% 57/313 [00:03<00:14, 17.63it/s]\u001b[A\n",
            " 19% 59/313 [00:03<00:14, 17.56it/s]\u001b[A\n",
            " 19% 61/313 [00:03<00:14, 17.66it/s]\u001b[A\n",
            " 20% 63/313 [00:03<00:14, 17.62it/s]\u001b[A\n",
            " 21% 65/313 [00:03<00:14, 17.54it/s]\u001b[A\n",
            " 21% 67/313 [00:03<00:13, 17.62it/s]\u001b[A\n",
            " 22% 69/313 [00:03<00:13, 17.51it/s]\u001b[A\n",
            " 23% 71/313 [00:03<00:13, 17.65it/s]\u001b[A\n",
            " 23% 73/313 [00:04<00:13, 17.69it/s]\u001b[A\n",
            " 24% 75/313 [00:04<00:13, 17.83it/s]\u001b[A\n",
            " 25% 77/313 [00:04<00:13, 17.70it/s]\u001b[A\n",
            " 25% 79/313 [00:04<00:13, 17.59it/s]\u001b[A\n",
            " 26% 81/313 [00:04<00:13, 17.67it/s]\u001b[A\n",
            " 27% 83/313 [00:04<00:13, 17.52it/s]\u001b[A\n",
            " 27% 85/313 [00:04<00:13, 17.53it/s]\u001b[A\n",
            " 28% 87/313 [00:04<00:12, 17.66it/s]\u001b[A\n",
            " 28% 89/313 [00:04<00:12, 17.55it/s]\u001b[A\n",
            " 29% 91/313 [00:05<00:12, 17.65it/s]\u001b[A\n",
            " 30% 93/313 [00:05<00:12, 17.71it/s]\u001b[A\n",
            " 30% 95/313 [00:05<00:12, 17.79it/s]\u001b[A\n",
            " 31% 97/313 [00:05<00:12, 17.68it/s]\u001b[A\n",
            " 32% 99/313 [00:05<00:12, 17.51it/s]\u001b[A\n",
            " 32% 101/313 [00:05<00:12, 17.63it/s]\u001b[A\n",
            " 33% 103/313 [00:05<00:11, 17.69it/s]\u001b[A\n",
            " 34% 105/313 [00:05<00:11, 17.87it/s]\u001b[A\n",
            " 34% 107/313 [00:06<00:11, 17.76it/s]\u001b[A\n",
            " 35% 109/313 [00:06<00:11, 17.63it/s]\u001b[A\n",
            " 35% 111/313 [00:06<00:11, 17.68it/s]\u001b[A\n",
            " 36% 113/313 [00:06<00:11, 17.62it/s]\u001b[A\n",
            " 37% 115/313 [00:06<00:11, 17.76it/s]\u001b[A\n",
            " 37% 117/313 [00:06<00:11, 17.61it/s]\u001b[A\n",
            " 38% 119/313 [00:06<00:11, 17.54it/s]\u001b[A\n",
            " 39% 121/313 [00:06<00:10, 17.59it/s]\u001b[A\n",
            " 39% 123/313 [00:06<00:10, 17.63it/s]\u001b[A\n",
            " 40% 125/313 [00:07<00:10, 17.70it/s]\u001b[A\n",
            " 41% 127/313 [00:07<00:10, 17.60it/s]\u001b[A\n",
            " 41% 129/313 [00:07<00:10, 17.64it/s]\u001b[A\n",
            " 42% 131/313 [00:07<00:10, 17.72it/s]\u001b[A\n",
            " 42% 133/313 [00:07<00:10, 17.54it/s]\u001b[A\n",
            " 43% 135/313 [00:07<00:10, 17.62it/s]\u001b[A\n",
            " 44% 137/313 [00:07<00:10, 17.59it/s]\u001b[A\n",
            " 44% 139/313 [00:07<00:09, 17.50it/s]\u001b[A\n",
            " 45% 141/313 [00:07<00:09, 17.59it/s]\u001b[A\n",
            " 46% 143/313 [00:08<00:09, 17.60it/s]\u001b[A\n",
            " 46% 145/313 [00:08<00:09, 17.51it/s]\u001b[A\n",
            " 47% 147/313 [00:08<00:09, 17.62it/s]\u001b[A\n",
            " 48% 149/313 [00:08<00:09, 17.58it/s]\u001b[A\n",
            " 48% 151/313 [00:08<00:09, 17.55it/s]\u001b[A\n",
            " 49% 153/313 [00:08<00:09, 17.63it/s]\u001b[A\n",
            " 50% 155/313 [00:08<00:09, 17.54it/s]\u001b[A\n",
            " 50% 157/313 [00:08<00:08, 17.70it/s]\u001b[A\n",
            " 51% 159/313 [00:08<00:08, 17.67it/s]\u001b[A\n",
            " 51% 161/313 [00:09<00:08, 17.79it/s]\u001b[A\n",
            " 52% 163/313 [00:09<00:08, 17.75it/s]\u001b[A\n",
            " 53% 165/313 [00:09<00:08, 17.85it/s]\u001b[A\n",
            " 53% 167/313 [00:09<00:08, 17.67it/s]\u001b[A\n",
            " 54% 169/313 [00:09<00:08, 17.67it/s]\u001b[A\n",
            " 55% 171/313 [00:09<00:08, 17.63it/s]\u001b[A\n",
            " 55% 173/313 [00:09<00:07, 17.58it/s]\u001b[A\n",
            " 56% 175/313 [00:09<00:07, 17.62it/s]\u001b[A\n",
            " 57% 177/313 [00:09<00:07, 17.66it/s]\u001b[A\n",
            " 57% 179/313 [00:10<00:07, 17.68it/s]\u001b[A\n",
            " 58% 181/313 [00:10<00:07, 17.71it/s]\u001b[A\n",
            " 58% 183/313 [00:10<00:07, 17.66it/s]\u001b[A\n",
            " 59% 185/313 [00:10<00:07, 17.72it/s]\u001b[A\n",
            " 60% 187/313 [00:10<00:07, 17.64it/s]\u001b[A\n",
            " 60% 189/313 [00:10<00:07, 17.55it/s]\u001b[A\n",
            " 61% 191/313 [00:10<00:06, 17.54it/s]\u001b[A\n",
            " 62% 193/313 [00:10<00:06, 17.53it/s]\u001b[A\n",
            " 62% 195/313 [00:11<00:06, 17.57it/s]\u001b[A\n",
            " 63% 197/313 [00:11<00:06, 17.68it/s]\u001b[A\n",
            " 64% 199/313 [00:11<00:06, 17.77it/s]\u001b[A\n",
            " 64% 201/313 [00:11<00:06, 17.85it/s]\u001b[A\n",
            " 65% 203/313 [00:11<00:06, 17.79it/s]\u001b[A\n",
            " 65% 205/313 [00:11<00:06, 17.96it/s]\u001b[A\n",
            " 66% 207/313 [00:11<00:05, 17.74it/s]\u001b[A\n",
            " 67% 209/313 [00:11<00:05, 17.82it/s]\u001b[A\n",
            " 67% 211/313 [00:11<00:05, 17.66it/s]\u001b[A\n",
            " 68% 213/313 [00:12<00:05, 17.62it/s]\u001b[A\n",
            " 69% 215/313 [00:12<00:05, 17.74it/s]\u001b[A\n",
            " 69% 217/313 [00:12<00:05, 17.72it/s]\u001b[A\n",
            " 70% 219/313 [00:12<00:05, 17.79it/s]\u001b[A\n",
            " 71% 221/313 [00:12<00:05, 17.73it/s]\u001b[A\n",
            " 71% 223/313 [00:12<00:05, 17.88it/s]\u001b[A\n",
            " 72% 225/313 [00:12<00:04, 17.73it/s]\u001b[A\n",
            " 73% 227/313 [00:12<00:04, 17.82it/s]\u001b[A\n",
            " 73% 229/313 [00:12<00:04, 17.73it/s]\u001b[A\n",
            " 74% 231/313 [00:13<00:04, 17.58it/s]\u001b[A\n",
            " 74% 233/313 [00:13<00:04, 17.69it/s]\u001b[A\n",
            " 75% 235/313 [00:13<00:04, 17.65it/s]\u001b[A\n",
            " 76% 237/313 [00:13<00:04, 17.80it/s]\u001b[A\n",
            " 76% 239/313 [00:13<00:04, 17.67it/s]\u001b[A\n",
            " 77% 241/313 [00:13<00:04, 17.69it/s]\u001b[A\n",
            " 78% 243/313 [00:13<00:03, 17.71it/s]\u001b[A\n",
            " 78% 245/313 [00:13<00:03, 17.56it/s]\u001b[A\n",
            " 79% 247/313 [00:13<00:03, 17.64it/s]\u001b[A\n",
            " 80% 249/313 [00:14<00:03, 17.60it/s]\u001b[A\n",
            " 33% 2500/7500 [04:19<08:13, 10.12it/s]\n",
            " 81% 253/313 [00:14<00:03, 17.68it/s]\u001b[A\n",
            " 81% 255/313 [00:14<00:03, 17.67it/s]\u001b[A\n",
            " 82% 257/313 [00:14<00:03, 17.67it/s]\u001b[A\n",
            " 83% 259/313 [00:14<00:03, 17.61it/s]\u001b[A\n",
            " 83% 261/313 [00:14<00:02, 17.62it/s]\u001b[A\n",
            " 84% 263/313 [00:14<00:02, 17.71it/s]\u001b[A\n",
            " 85% 265/313 [00:14<00:02, 17.67it/s]\u001b[A\n",
            " 85% 267/313 [00:15<00:02, 17.72it/s]\u001b[A\n",
            " 86% 269/313 [00:15<00:02, 17.65it/s]\u001b[A\n",
            " 87% 271/313 [00:15<00:02, 17.79it/s]\u001b[A\n",
            " 87% 273/313 [00:15<00:02, 17.71it/s]\u001b[A\n",
            " 88% 275/313 [00:15<00:02, 17.70it/s]\u001b[A\n",
            " 88% 277/313 [00:15<00:02, 17.68it/s]\u001b[A\n",
            " 89% 279/313 [00:15<00:01, 17.53it/s]\u001b[A\n",
            " 90% 281/313 [00:15<00:01, 17.60it/s]\u001b[A\n",
            " 90% 283/313 [00:15<00:01, 17.66it/s]\u001b[A\n",
            " 91% 285/313 [00:16<00:01, 17.81it/s]\u001b[A\n",
            " 92% 287/313 [00:16<00:01, 17.89it/s]\u001b[A\n",
            " 92% 289/313 [00:16<00:01, 18.00it/s]\u001b[A\n",
            " 93% 291/313 [00:16<00:01, 17.84it/s]\u001b[A\n",
            " 94% 293/313 [00:16<00:01, 17.78it/s]\u001b[A\n",
            " 94% 295/313 [00:16<00:01, 17.78it/s]\u001b[A\n",
            " 95% 297/313 [00:16<00:00, 17.52it/s]\u001b[A\n",
            " 96% 299/313 [00:16<00:00, 17.61it/s]\u001b[A\n",
            " 96% 301/313 [00:16<00:00, 17.59it/s]\u001b[A\n",
            " 97% 303/313 [00:17<00:00, 17.64it/s]\u001b[A\n",
            " 97% 305/313 [00:17<00:00, 17.71it/s]\u001b[A\n",
            " 98% 307/313 [00:17<00:00, 17.76it/s]\u001b[A\n",
            " 99% 309/313 [00:17<00:00, 17.78it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': '0.3805', 'eval_accuracy': '0.8646', 'eval_runtime': '17.69', 'eval_samples_per_second': '282.7', 'eval_steps_per_second': '17.7', 'epoch': '1'}\n",
            " 33% 2500/7500 [04:22<08:13, 10.12it/s]\n",
            "100% 313/313 [00:17<00:00, 17.68it/s]\u001b[A\n",
            "                                     \u001b[A\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:02<00:00,  2.90s/it]\n",
            "{'loss': '0.2908', 'grad_norm': '10.06', 'learning_rate': '3.267e-05', 'epoch': '1.04'}\n",
            "{'loss': '0.1959', 'grad_norm': '10.13', 'learning_rate': '3.201e-05', 'epoch': '1.08'}\n",
            "{'loss': '0.2922', 'grad_norm': '18.85', 'learning_rate': '3.134e-05', 'epoch': '1.12'}\n",
            "{'loss': '0.2584', 'grad_norm': '11.72', 'learning_rate': '3.067e-05', 'epoch': '1.16'}\n",
            "{'loss': '0.2408', 'grad_norm': '10.53', 'learning_rate': '3.001e-05', 'epoch': '1.2'}\n",
            "{'loss': '0.2775', 'grad_norm': '6.01', 'learning_rate': '2.934e-05', 'epoch': '1.24'}\n",
            "{'loss': '0.2403', 'grad_norm': '19.1', 'learning_rate': '2.867e-05', 'epoch': '1.28'}\n",
            "{'loss': '0.2527', 'grad_norm': '21.99', 'learning_rate': '2.801e-05', 'epoch': '1.32'}\n",
            "{'loss': '0.2353', 'grad_norm': '0.06374', 'learning_rate': '2.734e-05', 'epoch': '1.36'}\n",
            "{'loss': '0.2644', 'grad_norm': '20.18', 'learning_rate': '2.667e-05', 'epoch': '1.4'}\n",
            "{'loss': '0.1965', 'grad_norm': '15.21', 'learning_rate': '2.601e-05', 'epoch': '1.44'}\n",
            "{'loss': '0.2874', 'grad_norm': '36.99', 'learning_rate': '2.534e-05', 'epoch': '1.48'}\n",
            "{'loss': '0.207', 'grad_norm': '43.79', 'learning_rate': '2.467e-05', 'epoch': '1.52'}\n",
            "{'loss': '0.2576', 'grad_norm': '13.81', 'learning_rate': '2.401e-05', 'epoch': '1.56'}\n",
            "{'loss': '0.2306', 'grad_norm': '0.3505', 'learning_rate': '2.334e-05', 'epoch': '1.6'}\n",
            "{'loss': '0.2643', 'grad_norm': '15.68', 'learning_rate': '2.267e-05', 'epoch': '1.64'}\n",
            "{'loss': '0.2852', 'grad_norm': '25.29', 'learning_rate': '2.201e-05', 'epoch': '1.68'}\n",
            "{'loss': '0.2229', 'grad_norm': '0.1845', 'learning_rate': '2.134e-05', 'epoch': '1.72'}\n",
            "{'loss': '0.261', 'grad_norm': '2.108', 'learning_rate': '2.067e-05', 'epoch': '1.76'}\n",
            "{'loss': '0.2372', 'grad_norm': '53.93', 'learning_rate': '2.001e-05', 'epoch': '1.8'}\n",
            "{'loss': '0.2561', 'grad_norm': '5.332', 'learning_rate': '1.934e-05', 'epoch': '1.84'}\n",
            "{'loss': '0.2494', 'grad_norm': '7.894', 'learning_rate': '1.867e-05', 'epoch': '1.88'}\n",
            "{'loss': '0.1951', 'grad_norm': '11.85', 'learning_rate': '1.801e-05', 'epoch': '1.92'}\n",
            "{'loss': '0.2721', 'grad_norm': '0.5619', 'learning_rate': '1.734e-05', 'epoch': '1.96'}\n",
            "{'loss': '0.2295', 'grad_norm': '0.4491', 'learning_rate': '1.667e-05', 'epoch': '2'}\n",
            " 67% 5000/7500 [08:38<04:07, 10.10it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 3/313 [00:00<00:11, 26.27it/s]\u001b[A\n",
            "  2% 6/313 [00:00<00:15, 20.41it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:15, 19.15it/s]\u001b[A\n",
            "  4% 11/313 [00:00<00:16, 18.46it/s]\u001b[A\n",
            "  4% 13/313 [00:00<00:16, 18.01it/s]\u001b[A\n",
            "  5% 15/313 [00:00<00:16, 17.86it/s]\u001b[A\n",
            "  5% 17/313 [00:00<00:16, 17.71it/s]\u001b[A\n",
            "  6% 19/313 [00:01<00:16, 17.69it/s]\u001b[A\n",
            "  7% 21/313 [00:01<00:16, 17.91it/s]\u001b[A\n",
            "  7% 23/313 [00:01<00:16, 17.76it/s]\u001b[A\n",
            "  8% 25/313 [00:01<00:16, 17.81it/s]\u001b[A\n",
            "  9% 27/313 [00:01<00:16, 17.64it/s]\u001b[A\n",
            "  9% 29/313 [00:01<00:16, 17.30it/s]\u001b[A\n",
            " 10% 31/313 [00:01<00:16, 17.06it/s]\u001b[A\n",
            " 11% 33/313 [00:01<00:16, 16.99it/s]\u001b[A\n",
            " 11% 35/313 [00:01<00:16, 17.18it/s]\u001b[A\n",
            " 12% 37/313 [00:02<00:15, 17.37it/s]\u001b[A\n",
            " 12% 39/313 [00:02<00:15, 17.47it/s]\u001b[A\n",
            " 13% 41/313 [00:02<00:15, 17.66it/s]\u001b[A\n",
            " 14% 43/313 [00:02<00:15, 17.51it/s]\u001b[A\n",
            " 14% 45/313 [00:02<00:15, 17.60it/s]\u001b[A\n",
            " 15% 47/313 [00:02<00:15, 17.46it/s]\u001b[A\n",
            " 16% 49/313 [00:02<00:15, 17.33it/s]\u001b[A\n",
            " 16% 51/313 [00:02<00:15, 17.30it/s]\u001b[A\n",
            " 17% 53/313 [00:02<00:14, 17.47it/s]\u001b[A\n",
            " 18% 55/313 [00:03<00:14, 17.46it/s]\u001b[A\n",
            " 18% 57/313 [00:03<00:14, 17.61it/s]\u001b[A\n",
            " 19% 59/313 [00:03<00:14, 17.68it/s]\u001b[A\n",
            " 19% 61/313 [00:03<00:14, 17.79it/s]\u001b[A\n",
            " 20% 63/313 [00:03<00:14, 17.73it/s]\u001b[A\n",
            " 21% 65/313 [00:03<00:14, 17.61it/s]\u001b[A\n",
            " 21% 67/313 [00:03<00:14, 17.57it/s]\u001b[A\n",
            " 22% 69/313 [00:03<00:14, 17.40it/s]\u001b[A\n",
            " 23% 71/313 [00:04<00:13, 17.38it/s]\u001b[A\n",
            " 23% 73/313 [00:04<00:13, 17.45it/s]\u001b[A\n",
            " 24% 75/313 [00:04<00:13, 17.52it/s]\u001b[A\n",
            " 25% 77/313 [00:04<00:13, 17.63it/s]\u001b[A\n",
            " 25% 79/313 [00:04<00:13, 17.77it/s]\u001b[A\n",
            " 26% 81/313 [00:04<00:13, 17.67it/s]\u001b[A\n",
            " 27% 83/313 [00:04<00:13, 17.69it/s]\u001b[A\n",
            " 27% 85/313 [00:04<00:12, 17.60it/s]\u001b[A\n",
            " 28% 87/313 [00:04<00:12, 17.49it/s]\u001b[A\n",
            " 28% 89/313 [00:05<00:12, 17.51it/s]\u001b[A\n",
            " 29% 91/313 [00:05<00:12, 17.54it/s]\u001b[A\n",
            " 30% 93/313 [00:05<00:12, 17.55it/s]\u001b[A\n",
            " 30% 95/313 [00:05<00:12, 17.71it/s]\u001b[A\n",
            " 31% 97/313 [00:05<00:12, 17.70it/s]\u001b[A\n",
            " 32% 99/313 [00:05<00:12, 17.75it/s]\u001b[A\n",
            " 32% 101/313 [00:05<00:12, 17.55it/s]\u001b[A\n",
            " 33% 103/313 [00:05<00:11, 17.59it/s]\u001b[A\n",
            " 34% 105/313 [00:05<00:11, 17.46it/s]\u001b[A\n",
            " 34% 107/313 [00:06<00:11, 17.50it/s]\u001b[A\n",
            " 35% 109/313 [00:06<00:11, 17.59it/s]\u001b[A\n",
            " 35% 111/313 [00:06<00:11, 17.57it/s]\u001b[A\n",
            " 36% 113/313 [00:06<00:11, 17.43it/s]\u001b[A\n",
            " 37% 115/313 [00:06<00:11, 17.58it/s]\u001b[A\n",
            " 37% 117/313 [00:06<00:11, 17.59it/s]\u001b[A\n",
            " 38% 119/313 [00:06<00:10, 17.66it/s]\u001b[A\n",
            " 39% 121/313 [00:06<00:10, 17.58it/s]\u001b[A\n",
            " 39% 123/313 [00:06<00:10, 17.48it/s]\u001b[A\n",
            " 40% 125/313 [00:07<00:10, 17.67it/s]\u001b[A\n",
            " 41% 127/313 [00:07<00:10, 17.61it/s]\u001b[A\n",
            " 41% 129/313 [00:07<00:10, 17.60it/s]\u001b[A\n",
            " 42% 131/313 [00:07<00:10, 17.66it/s]\u001b[A\n",
            " 42% 133/313 [00:07<00:10, 17.53it/s]\u001b[A\n",
            " 43% 135/313 [00:07<00:10, 17.65it/s]\u001b[A\n",
            " 44% 137/313 [00:07<00:09, 17.70it/s]\u001b[A\n",
            " 44% 139/313 [00:07<00:09, 17.69it/s]\u001b[A\n",
            " 45% 141/313 [00:07<00:09, 17.59it/s]\u001b[A\n",
            " 46% 143/313 [00:08<00:09, 17.54it/s]\u001b[A\n",
            " 46% 145/313 [00:08<00:09, 17.59it/s]\u001b[A\n",
            " 47% 147/313 [00:08<00:09, 17.58it/s]\u001b[A\n",
            " 48% 149/313 [00:08<00:09, 17.67it/s]\u001b[A\n",
            " 48% 151/313 [00:08<00:09, 17.65it/s]\u001b[A\n",
            " 49% 153/313 [00:08<00:09, 17.60it/s]\u001b[A\n",
            " 50% 155/313 [00:08<00:08, 17.65it/s]\u001b[A\n",
            " 50% 157/313 [00:08<00:08, 17.64it/s]\u001b[A\n",
            " 51% 159/313 [00:09<00:08, 17.60it/s]\u001b[A\n",
            " 51% 161/313 [00:09<00:08, 17.63it/s]\u001b[A\n",
            " 52% 163/313 [00:09<00:08, 17.55it/s]\u001b[A\n",
            " 53% 165/313 [00:09<00:08, 17.56it/s]\u001b[A\n",
            " 53% 167/313 [00:09<00:08, 17.58it/s]\u001b[A\n",
            " 54% 169/313 [00:09<00:08, 17.53it/s]\u001b[A\n",
            " 55% 171/313 [00:09<00:08, 17.67it/s]\u001b[A\n",
            " 55% 173/313 [00:09<00:07, 17.57it/s]\u001b[A\n",
            " 56% 175/313 [00:09<00:07, 17.52it/s]\u001b[A\n",
            " 57% 177/313 [00:10<00:07, 17.60it/s]\u001b[A\n",
            " 57% 179/313 [00:10<00:07, 17.56it/s]\u001b[A\n",
            " 58% 181/313 [00:10<00:07, 17.59it/s]\u001b[A\n",
            " 67% 5000/7500 [08:49<04:07, 10.10it/s]\n",
            " 59% 185/313 [00:10<00:07, 17.68it/s]\u001b[A\n",
            " 60% 187/313 [00:10<00:07, 17.67it/s]\u001b[A\n",
            " 60% 189/313 [00:10<00:07, 17.55it/s]\u001b[A\n",
            " 61% 191/313 [00:10<00:06, 17.57it/s]\u001b[A\n",
            " 62% 193/313 [00:10<00:06, 17.54it/s]\u001b[A\n",
            " 62% 195/313 [00:11<00:06, 17.57it/s]\u001b[A\n",
            " 63% 197/313 [00:11<00:06, 17.64it/s]\u001b[A\n",
            " 64% 199/313 [00:11<00:06, 17.59it/s]\u001b[A\n",
            " 64% 201/313 [00:11<00:06, 17.43it/s]\u001b[A\n",
            " 65% 203/313 [00:11<00:06, 17.63it/s]\u001b[A\n",
            " 65% 205/313 [00:11<00:06, 17.58it/s]\u001b[A\n",
            " 66% 207/313 [00:11<00:06, 17.50it/s]\u001b[A\n",
            " 67% 209/313 [00:11<00:05, 17.53it/s]\u001b[A\n",
            " 67% 211/313 [00:11<00:05, 17.48it/s]\u001b[A\n",
            " 68% 213/313 [00:12<00:05, 17.50it/s]\u001b[A\n",
            " 69% 215/313 [00:12<00:05, 17.56it/s]\u001b[A\n",
            " 69% 217/313 [00:12<00:05, 17.45it/s]\u001b[A\n",
            " 70% 219/313 [00:12<00:05, 17.57it/s]\u001b[A\n",
            " 71% 221/313 [00:12<00:05, 17.60it/s]\u001b[A\n",
            " 71% 223/313 [00:12<00:05, 17.60it/s]\u001b[A\n",
            " 72% 225/313 [00:12<00:05, 17.51it/s]\u001b[A\n",
            " 73% 227/313 [00:12<00:04, 17.53it/s]\u001b[A\n",
            " 73% 229/313 [00:12<00:04, 17.64it/s]\u001b[A\n",
            " 74% 231/313 [00:13<00:04, 17.60it/s]\u001b[A\n",
            " 74% 233/313 [00:13<00:04, 17.59it/s]\u001b[A\n",
            " 75% 235/313 [00:13<00:04, 17.48it/s]\u001b[A\n",
            " 76% 237/313 [00:13<00:04, 17.42it/s]\u001b[A\n",
            " 76% 239/313 [00:13<00:04, 17.42it/s]\u001b[A\n",
            " 77% 241/313 [00:13<00:04, 17.58it/s]\u001b[A\n",
            " 78% 243/313 [00:13<00:04, 17.46it/s]\u001b[A\n",
            " 78% 245/313 [00:13<00:03, 17.52it/s]\u001b[A\n",
            " 79% 247/313 [00:14<00:03, 17.62it/s]\u001b[A\n",
            " 80% 249/313 [00:14<00:03, 17.58it/s]\u001b[A\n",
            " 80% 251/313 [00:14<00:03, 17.59it/s]\u001b[A\n",
            " 81% 253/313 [00:14<00:03, 17.48it/s]\u001b[A\n",
            " 81% 255/313 [00:14<00:03, 17.42it/s]\u001b[A\n",
            " 82% 257/313 [00:14<00:03, 17.55it/s]\u001b[A\n",
            " 83% 259/313 [00:14<00:03, 17.56it/s]\u001b[A\n",
            " 83% 261/313 [00:14<00:02, 17.71it/s]\u001b[A\n",
            " 84% 263/313 [00:14<00:02, 17.61it/s]\u001b[A\n",
            " 85% 265/313 [00:15<00:02, 17.70it/s]\u001b[A\n",
            " 85% 267/313 [00:15<00:02, 17.60it/s]\u001b[A\n",
            " 86% 269/313 [00:15<00:02, 17.60it/s]\u001b[A\n",
            " 87% 271/313 [00:15<00:02, 17.64it/s]\u001b[A\n",
            " 87% 273/313 [00:15<00:02, 17.43it/s]\u001b[A\n",
            " 88% 275/313 [00:15<00:02, 17.39it/s]\u001b[A\n",
            " 88% 277/313 [00:15<00:02, 17.59it/s]\u001b[A\n",
            " 89% 279/313 [00:15<00:01, 17.54it/s]\u001b[A\n",
            " 90% 281/313 [00:15<00:01, 17.54it/s]\u001b[A\n",
            " 90% 283/313 [00:16<00:01, 17.50it/s]\u001b[A\n",
            " 91% 285/313 [00:16<00:01, 17.44it/s]\u001b[A\n",
            " 92% 287/313 [00:16<00:01, 17.47it/s]\u001b[A\n",
            " 92% 289/313 [00:16<00:01, 17.53it/s]\u001b[A\n",
            " 93% 291/313 [00:16<00:01, 17.54it/s]\u001b[A\n",
            " 94% 293/313 [00:16<00:01, 17.59it/s]\u001b[A\n",
            " 94% 295/313 [00:16<00:01, 17.62it/s]\u001b[A\n",
            " 95% 297/313 [00:16<00:00, 17.64it/s]\u001b[A\n",
            " 96% 299/313 [00:16<00:00, 17.55it/s]\u001b[A\n",
            " 96% 301/313 [00:17<00:00, 17.45it/s]\u001b[A\n",
            " 97% 303/313 [00:17<00:00, 17.55it/s]\u001b[A\n",
            " 97% 305/313 [00:17<00:00, 17.38it/s]\u001b[A\n",
            " 98% 307/313 [00:17<00:00, 17.41it/s]\u001b[A\n",
            " 99% 309/313 [00:17<00:00, 17.59it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': '0.4556', 'eval_accuracy': '0.8692', 'eval_runtime': '17.8', 'eval_samples_per_second': '280.9', 'eval_steps_per_second': '17.59', 'epoch': '2'}\n",
            " 67% 5000/7500 [08:56<04:07, 10.10it/s]\n",
            "100% 313/313 [00:17<00:00, 17.47it/s]\u001b[A\n",
            "                                     \u001b[A\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:00<00:00,  1.49it/s]\n",
            "{'loss': '0.1356', 'grad_norm': '0.8039', 'learning_rate': '1.601e-05', 'epoch': '2.04'}\n",
            "{'loss': '0.1111', 'grad_norm': '42.18', 'learning_rate': '1.534e-05', 'epoch': '2.08'}\n",
            "{'loss': '0.1162', 'grad_norm': '0.08089', 'learning_rate': '1.467e-05', 'epoch': '2.12'}\n",
            "{'loss': '0.1207', 'grad_norm': '0.1053', 'learning_rate': '1.401e-05', 'epoch': '2.16'}\n",
            "{'loss': '0.1024', 'grad_norm': '15.92', 'learning_rate': '1.334e-05', 'epoch': '2.2'}\n",
            "{'loss': '0.1246', 'grad_norm': '13.86', 'learning_rate': '1.267e-05', 'epoch': '2.24'}\n",
            "{'loss': '0.1057', 'grad_norm': '42.78', 'learning_rate': '1.201e-05', 'epoch': '2.28'}\n",
            "{'loss': '0.12', 'grad_norm': '32.61', 'learning_rate': '1.134e-05', 'epoch': '2.32'}\n",
            "{'loss': '0.1197', 'grad_norm': '3.081', 'learning_rate': '1.067e-05', 'epoch': '2.36'}\n",
            "{'loss': '0.07867', 'grad_norm': '0.01246', 'learning_rate': '1.001e-05', 'epoch': '2.4'}\n",
            "{'loss': '0.1043', 'grad_norm': '0.4164', 'learning_rate': '9.34e-06', 'epoch': '2.44'}\n",
            "{'loss': '0.09811', 'grad_norm': '0.1027', 'learning_rate': '8.673e-06', 'epoch': '2.48'}\n",
            "{'loss': '0.1121', 'grad_norm': '2.532', 'learning_rate': '8.007e-06', 'epoch': '2.52'}\n",
            "{'loss': '0.08397', 'grad_norm': '0.03006', 'learning_rate': '7.34e-06', 'epoch': '2.56'}\n",
            "{'loss': '0.08057', 'grad_norm': '0.01988', 'learning_rate': '6.673e-06', 'epoch': '2.6'}\n",
            "{'loss': '0.1138', 'grad_norm': '5.326', 'learning_rate': '6.007e-06', 'epoch': '2.64'}\n",
            "{'loss': '0.07506', 'grad_norm': '0.07757', 'learning_rate': '5.34e-06', 'epoch': '2.68'}\n",
            "{'loss': '0.142', 'grad_norm': '0.3677', 'learning_rate': '4.673e-06', 'epoch': '2.72'}\n",
            "{'loss': '0.07544', 'grad_norm': '0.02418', 'learning_rate': '4.007e-06', 'epoch': '2.76'}\n",
            "{'loss': '0.1296', 'grad_norm': '0.03503', 'learning_rate': '3.34e-06', 'epoch': '2.8'}\n",
            "{'loss': '0.07903', 'grad_norm': '0.02055', 'learning_rate': '2.673e-06', 'epoch': '2.84'}\n",
            "{'loss': '0.1159', 'grad_norm': '0.03156', 'learning_rate': '2.007e-06', 'epoch': '2.88'}\n",
            "{'loss': '0.08928', 'grad_norm': '0.1242', 'learning_rate': '1.34e-06', 'epoch': '2.92'}\n",
            "{'loss': '0.08757', 'grad_norm': '0.03784', 'learning_rate': '6.733e-07', 'epoch': '2.96'}\n",
            "{'loss': '0.09876', 'grad_norm': '12.28', 'learning_rate': '6.667e-09', 'epoch': '3'}\n",
            "100% 7500/7500 [13:08<00:00, 10.09it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 3/313 [00:00<00:12, 25.08it/s]\u001b[A\n",
            "  2% 6/313 [00:00<00:15, 20.18it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:16, 18.94it/s]\u001b[A\n",
            "  4% 11/313 [00:00<00:16, 18.53it/s]\u001b[A\n",
            "  4% 13/313 [00:00<00:16, 18.12it/s]\u001b[A\n",
            "  5% 15/313 [00:00<00:16, 17.84it/s]\u001b[A\n",
            "  5% 17/313 [00:00<00:16, 17.56it/s]\u001b[A\n",
            "  6% 19/313 [00:01<00:16, 17.74it/s]\u001b[A\n",
            "  7% 21/313 [00:01<00:16, 17.66it/s]\u001b[A\n",
            "  7% 23/313 [00:01<00:16, 17.70it/s]\u001b[A\n",
            "  8% 25/313 [00:01<00:16, 17.73it/s]\u001b[A\n",
            "  9% 27/313 [00:01<00:16, 17.53it/s]\u001b[A\n",
            "  9% 29/313 [00:01<00:16, 17.69it/s]\u001b[A\n",
            " 10% 31/313 [00:01<00:16, 17.54it/s]\u001b[A\n",
            " 11% 33/313 [00:01<00:16, 17.39it/s]\u001b[A\n",
            " 11% 35/313 [00:01<00:15, 17.51it/s]\u001b[A\n",
            " 12% 37/313 [00:02<00:15, 17.56it/s]\u001b[A\n",
            " 12% 39/313 [00:02<00:15, 17.53it/s]\u001b[A\n",
            " 13% 41/313 [00:02<00:15, 17.65it/s]\u001b[A\n",
            " 14% 43/313 [00:02<00:15, 17.51it/s]\u001b[A\n",
            " 14% 45/313 [00:02<00:15, 17.61it/s]\u001b[A\n",
            " 15% 47/313 [00:02<00:15, 17.58it/s]\u001b[A\n",
            " 16% 49/313 [00:02<00:15, 17.32it/s]\u001b[A\n",
            " 16% 51/313 [00:02<00:15, 17.37it/s]\u001b[A\n",
            " 17% 53/313 [00:02<00:14, 17.53it/s]\u001b[A\n",
            " 18% 55/313 [00:03<00:14, 17.55it/s]\u001b[A\n",
            " 18% 57/313 [00:03<00:14, 17.66it/s]\u001b[A\n",
            " 19% 59/313 [00:03<00:14, 17.56it/s]\u001b[A\n",
            " 19% 61/313 [00:03<00:14, 17.34it/s]\u001b[A\n",
            " 20% 63/313 [00:03<00:14, 17.52it/s]\u001b[A\n",
            " 21% 65/313 [00:03<00:14, 17.46it/s]\u001b[A\n",
            " 21% 67/313 [00:03<00:14, 17.25it/s]\u001b[A\n",
            " 22% 69/313 [00:03<00:14, 17.29it/s]\u001b[A\n",
            " 23% 71/313 [00:04<00:13, 17.47it/s]\u001b[A\n",
            " 23% 73/313 [00:04<00:13, 17.43it/s]\u001b[A\n",
            " 24% 75/313 [00:04<00:13, 17.63it/s]\u001b[A\n",
            " 25% 77/313 [00:04<00:13, 17.59it/s]\u001b[A\n",
            " 25% 79/313 [00:04<00:13, 17.42it/s]\u001b[A\n",
            " 26% 81/313 [00:04<00:13, 17.63it/s]\u001b[A\n",
            " 27% 83/313 [00:04<00:13, 17.56it/s]\u001b[A\n",
            " 27% 85/313 [00:04<00:12, 17.55it/s]\u001b[A\n",
            " 28% 87/313 [00:04<00:12, 17.56it/s]\u001b[A\n",
            " 28% 89/313 [00:05<00:12, 17.43it/s]\u001b[A\n",
            " 29% 91/313 [00:05<00:12, 17.49it/s]\u001b[A\n",
            " 30% 93/313 [00:05<00:12, 17.56it/s]\u001b[A\n",
            " 30% 95/313 [00:05<00:12, 17.63it/s]\u001b[A\n",
            " 31% 97/313 [00:05<00:12, 17.61it/s]\u001b[A\n",
            " 32% 99/313 [00:05<00:12, 17.60it/s]\u001b[A\n",
            " 32% 101/313 [00:05<00:12, 17.60it/s]\u001b[A\n",
            " 33% 103/313 [00:05<00:11, 17.57it/s]\u001b[A\n",
            " 34% 105/313 [00:05<00:11, 17.51it/s]\u001b[A\n",
            " 34% 107/313 [00:06<00:11, 17.57it/s]\u001b[A\n",
            " 35% 109/313 [00:06<00:11, 17.54it/s]\u001b[A\n",
            " 35% 111/313 [00:06<00:11, 17.50it/s]\u001b[A\n",
            " 36% 113/313 [00:06<00:11, 17.54it/s]\u001b[A\n",
            " 37% 115/313 [00:06<00:11, 17.56it/s]\u001b[A\n",
            " 37% 117/313 [00:06<00:11, 17.61it/s]\u001b[A\n",
            " 38% 119/313 [00:06<00:11, 17.46it/s]\u001b[A\n",
            " 39% 121/313 [00:06<00:10, 17.47it/s]\u001b[A\n",
            " 39% 123/313 [00:06<00:10, 17.51it/s]\u001b[A\n",
            " 40% 125/313 [00:07<00:10, 17.56it/s]\u001b[A\n",
            " 41% 127/313 [00:07<00:10, 17.56it/s]\u001b[A\n",
            " 41% 129/313 [00:07<00:10, 17.71it/s]\u001b[A\n",
            " 42% 131/313 [00:07<00:10, 17.77it/s]\u001b[A\n",
            " 42% 133/313 [00:07<00:10, 17.75it/s]\u001b[A\n",
            " 43% 135/313 [00:07<00:10, 17.63it/s]\u001b[A\n",
            " 44% 137/313 [00:07<00:10, 17.56it/s]\u001b[A\n",
            " 44% 139/313 [00:07<00:09, 17.59it/s]\u001b[A\n",
            " 45% 141/313 [00:07<00:09, 17.61it/s]\u001b[A\n",
            " 46% 143/313 [00:08<00:09, 17.72it/s]\u001b[A\n",
            " 46% 145/313 [00:08<00:09, 17.59it/s]\u001b[A\n",
            " 47% 147/313 [00:08<00:09, 17.55it/s]\u001b[A\n",
            " 48% 149/313 [00:08<00:09, 17.60it/s]\u001b[A\n",
            " 48% 151/313 [00:08<00:09, 17.63it/s]\u001b[A\n",
            " 49% 153/313 [00:08<00:09, 17.74it/s]\u001b[A\n",
            " 50% 155/313 [00:08<00:08, 17.68it/s]\u001b[A\n",
            " 50% 157/313 [00:08<00:08, 17.59it/s]\u001b[A\n",
            " 51% 159/313 [00:09<00:08, 17.54it/s]\u001b[A\n",
            " 51% 161/313 [00:09<00:08, 17.52it/s]\u001b[A\n",
            " 52% 163/313 [00:09<00:08, 17.46it/s]\u001b[A\n",
            " 53% 165/313 [00:09<00:08, 17.72it/s]\u001b[A\n",
            " 53% 167/313 [00:09<00:08, 17.64it/s]\u001b[A\n",
            " 54% 169/313 [00:09<00:08, 17.68it/s]\u001b[A\n",
            " 55% 171/313 [00:09<00:08, 17.58it/s]\u001b[A\n",
            " 55% 173/313 [00:09<00:08, 17.49it/s]\u001b[A\n",
            " 56% 175/313 [00:09<00:07, 17.52it/s]\u001b[A\n",
            " 57% 177/313 [00:10<00:07, 17.55it/s]\u001b[A\n",
            " 57% 179/313 [00:10<00:07, 17.64it/s]\u001b[A\n",
            " 58% 181/313 [00:10<00:07, 17.43it/s]\u001b[A\n",
            " 58% 183/313 [00:10<00:07, 17.38it/s]\u001b[A\n",
            " 59% 185/313 [00:10<00:07, 17.44it/s]\u001b[A\n",
            "100% 7500/7500 [13:19<00:00, 10.09it/s]\n",
            " 60% 189/313 [00:10<00:07, 17.48it/s]\u001b[A\n",
            " 61% 191/313 [00:10<00:06, 17.66it/s]\u001b[A\n",
            " 62% 193/313 [00:10<00:06, 17.60it/s]\u001b[A\n",
            " 62% 195/313 [00:11<00:06, 17.60it/s]\u001b[A\n",
            " 63% 197/313 [00:11<00:06, 17.54it/s]\u001b[A\n",
            " 64% 199/313 [00:11<00:06, 17.49it/s]\u001b[A\n",
            " 64% 201/313 [00:11<00:06, 17.57it/s]\u001b[A\n",
            " 65% 203/313 [00:11<00:06, 17.51it/s]\u001b[A\n",
            " 65% 205/313 [00:11<00:06, 17.42it/s]\u001b[A\n",
            " 66% 207/313 [00:11<00:06, 17.53it/s]\u001b[A\n",
            " 67% 209/313 [00:11<00:05, 17.53it/s]\u001b[A\n",
            " 67% 211/313 [00:11<00:05, 17.39it/s]\u001b[A\n",
            " 68% 213/313 [00:12<00:05, 17.56it/s]\u001b[A\n",
            " 69% 215/313 [00:12<00:05, 17.56it/s]\u001b[A\n",
            " 69% 217/313 [00:12<00:05, 17.56it/s]\u001b[A\n",
            " 70% 219/313 [00:12<00:05, 17.54it/s]\u001b[A\n",
            " 71% 221/313 [00:12<00:05, 17.49it/s]\u001b[A\n",
            " 71% 223/313 [00:12<00:05, 17.60it/s]\u001b[A\n",
            " 72% 225/313 [00:12<00:05, 17.53it/s]\u001b[A\n",
            " 73% 227/313 [00:12<00:04, 17.59it/s]\u001b[A\n",
            " 73% 229/313 [00:12<00:04, 17.64it/s]\u001b[A\n",
            " 74% 231/313 [00:13<00:04, 17.60it/s]\u001b[A\n",
            " 74% 233/313 [00:13<00:04, 17.72it/s]\u001b[A\n",
            " 75% 235/313 [00:13<00:04, 17.67it/s]\u001b[A\n",
            " 76% 237/313 [00:13<00:04, 17.66it/s]\u001b[A\n",
            " 76% 239/313 [00:13<00:04, 17.69it/s]\u001b[A\n",
            " 77% 241/313 [00:13<00:04, 17.78it/s]\u001b[A\n",
            " 78% 243/313 [00:13<00:03, 17.82it/s]\u001b[A\n",
            " 78% 245/313 [00:13<00:03, 17.62it/s]\u001b[A\n",
            " 79% 247/313 [00:14<00:03, 17.69it/s]\u001b[A\n",
            " 80% 249/313 [00:14<00:03, 17.56it/s]\u001b[A\n",
            " 80% 251/313 [00:14<00:03, 17.59it/s]\u001b[A\n",
            " 81% 253/313 [00:14<00:03, 17.61it/s]\u001b[A\n",
            " 81% 255/313 [00:14<00:03, 17.53it/s]\u001b[A\n",
            " 82% 257/313 [00:14<00:03, 17.51it/s]\u001b[A\n",
            " 83% 259/313 [00:14<00:03, 17.59it/s]\u001b[A\n",
            " 83% 261/313 [00:14<00:02, 17.50it/s]\u001b[A\n",
            " 84% 263/313 [00:14<00:02, 17.62it/s]\u001b[A\n",
            " 85% 265/313 [00:15<00:02, 17.60it/s]\u001b[A\n",
            " 85% 267/313 [00:15<00:02, 17.75it/s]\u001b[A\n",
            " 86% 269/313 [00:15<00:02, 17.61it/s]\u001b[A\n",
            " 87% 271/313 [00:15<00:02, 17.53it/s]\u001b[A\n",
            " 87% 273/313 [00:15<00:02, 17.53it/s]\u001b[A\n",
            " 88% 275/313 [00:15<00:02, 17.58it/s]\u001b[A\n",
            " 88% 277/313 [00:15<00:02, 17.59it/s]\u001b[A\n",
            " 89% 279/313 [00:15<00:01, 17.53it/s]\u001b[A\n",
            " 90% 281/313 [00:15<00:01, 17.55it/s]\u001b[A\n",
            " 90% 283/313 [00:16<00:01, 17.72it/s]\u001b[A\n",
            " 91% 285/313 [00:16<00:01, 17.70it/s]\u001b[A\n",
            " 92% 287/313 [00:16<00:01, 17.69it/s]\u001b[A\n",
            " 92% 289/313 [00:16<00:01, 17.65it/s]\u001b[A\n",
            " 93% 291/313 [00:16<00:01, 17.48it/s]\u001b[A\n",
            " 94% 293/313 [00:16<00:01, 17.60it/s]\u001b[A\n",
            " 94% 295/313 [00:16<00:01, 17.53it/s]\u001b[A\n",
            " 95% 297/313 [00:16<00:00, 17.44it/s]\u001b[A\n",
            " 96% 299/313 [00:16<00:00, 17.57it/s]\u001b[A\n",
            " 96% 301/313 [00:17<00:00, 17.50it/s]\u001b[A\n",
            " 97% 303/313 [00:17<00:00, 17.42it/s]\u001b[A\n",
            " 97% 305/313 [00:17<00:00, 17.51it/s]\u001b[A\n",
            " 98% 307/313 [00:17<00:00, 17.54it/s]\u001b[A\n",
            " 99% 309/313 [00:17<00:00, 17.57it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': '0.6302', 'eval_accuracy': '0.8742', 'eval_runtime': '17.8', 'eval_samples_per_second': '280.9', 'eval_steps_per_second': '17.59', 'epoch': '3'}\n",
            "100% 7500/7500 [13:26<00:00, 10.09it/s]\n",
            "100% 313/313 [00:17<00:00, 17.44it/s]\u001b[A\n",
            "                                     \u001b[A\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:01<00:00,  1.73s/it]\n",
            "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
            "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n",
            "{'train_runtime': '811.4', 'train_samples_per_second': '73.94', 'train_steps_per_second': '9.243', 'train_loss': '0.2513', 'epoch': '3'}\n",
            "100% 7500/7500 [13:31<00:00,  9.24it/s]\n",
            "--- 4. Saving Model ---\n",
            "Writing model shards: 100% 1/1 [00:02<00:00,  2.90s/it]\n",
            "Model saved to ./models/bert_sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text_classifier.py\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "def train_topic_classifier():\n",
        "    print(\"--- 1. Loading 20 Newsgroups Dataset ---\")\n",
        "    # We pick 3 specific categories to make it clear\n",
        "    categories = ['sci.space', 'sci.med', 'comp.graphics']\n",
        "\n",
        "    train_data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "    test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "    print(f\"Categories: {train_data.target_names}\")\n",
        "    print(f\"Training samples: {len(train_data.data)}\")\n",
        "\n",
        "    print(\"--- 2. Building Pipeline (TF-IDF + Naive Bayes) ---\")\n",
        "    # Naive Bayes is excellent for text classification\n",
        "    model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "    print(\"--- 3. Training ---\")\n",
        "    model.fit(train_data.data, train_data.target)\n",
        "\n",
        "    print(\"--- 4. Evaluation ---\")\n",
        "    preds = model.predict(test_data.data)\n",
        "    acc = accuracy_score(test_data.target, preds)\n",
        "    print(f\"Multi-Class Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    # Save\n",
        "    joblib.dump(model, 'models/topic_classifier.pkl')\n",
        "    print(\"Topic Classifier saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_topic_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWn7oOdbWlNw",
        "outputId": "96dcc8e0-8abc-46c7-fbbb-857c52a5c5a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing text_classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python text_classifier.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K45LFaDZW0Ig",
        "outputId": "71162c49-3fa0-47fd-d483-348edb3c5a0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Loading 20 Newsgroups Dataset ---\n",
            "Categories: ['comp.graphics', 'sci.med', 'sci.space']\n",
            "Training samples: 1771\n",
            "--- 2. Building Pipeline (TF-IDF + Naive Bayes) ---\n",
            "--- 3. Training ---\n",
            "--- 4. Evaluation ---\n",
            "Multi-Class Accuracy: 94.06%\n",
            "Topic Classifier saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile inference_api.py\n",
        "import joblib\n",
        "from text_preprocessing import clean_text\n",
        "import sys\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    try:\n",
        "        model = joblib.load('models/classic_sentiment_model.pkl')\n",
        "        vectorizer = joblib.load('models/tfidf_vectorizer.pkl')\n",
        "\n",
        "        # Preprocess\n",
        "        clean = clean_text(text)\n",
        "        # Vectorize\n",
        "        vec = vectorizer.transform([clean])\n",
        "        # Predict\n",
        "        prob = model.predict_proba(vec)[0]\n",
        "        pred = model.predict(vec)[0]\n",
        "\n",
        "        label = \"POSITIVE\" if pred == 1 else \"NEGATIVE\"\n",
        "        confidence = prob[pred]\n",
        "\n",
        "        return f\"Sentiment: {label} ({confidence*100:.1f}%)\"\n",
        "    except Exception as e:\n",
        "        return \"Model not found. Run sentiment_analyzer.py first.\"\n",
        "\n",
        "def predict_topic(text):\n",
        "    try:\n",
        "        model = joblib.load('models/topic_classifier.pkl')\n",
        "        categories = ['Graphics', 'Medicine', 'Space'] # The order matches target_names\n",
        "\n",
        "        pred_idx = model.predict([text])[0]\n",
        "        return f\"Topic: {categories[pred_idx]}\"\n",
        "    except:\n",
        "        return \"Topic Model not found.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- AI Text Analyst ---\")\n",
        "    sample_text = \"The doctor said the surgery was successful.\"\n",
        "    print(f\"Input: {sample_text}\")\n",
        "    print(predict_sentiment(sample_text))\n",
        "    print(predict_topic(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpVsZKFWXBSt",
        "outputId": "2b45174e-4e73-4f3e-dbc8-0c480065cd35"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing inference_api.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference_api.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Xtdlw6oXH6D",
        "outputId": "a71c5e34-9c88-4042-d249-f70f6e173ab7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- AI Text Analyst ---\n",
            "Input: The doctor said the surgery was successful.\n",
            "Sentiment: NEGATIVE (74.7%)\n",
            "Topic: Medicine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile README.md\n",
        "# Day 5: NLP & Text Analysis\n",
        "\n",
        "## Project Overview\n",
        "This project implements Natural Language Processing (NLP) techniques to analyze text data. It includes a classic TF-IDF model, a state-of-the-art BERT model, and a multi-class topic classifier.\n",
        "\n",
        "## Deliverables\n",
        "1. **text_preprocessing.py**: Pipeline for cleaning and lemmatizing text.\n",
        "2. **sentiment_analyzer.py**: Logistic Regression model achieving ~85% on IMDB data.\n",
        "3. **transformer_model.py**: DistilBERT model for deep context understanding.\n",
        "4. **text_classifier.py**: Classification system for News topics (Space, Med, Graphics).\n",
        "5. **inference_api.py**: Simple script to test the models with new inputs.\n",
        "\n",
        "## How to Run\n",
        "1. Install dependencies: `pip install -r requirements.txt`\n",
        "2. Train Sentiment Model: `python sentiment_analyzer.py`\n",
        "3. Train BERT Model: `python transformer_model.py`\n",
        "4. Train Topic Classifier: `python text_classifier.py`\n",
        "5. Test Predictions: `python inference_api.py`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptdvsbxfXTNP",
        "outputId": "a2383cfd-1877-479e-80bf-02e8567bd760"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing README.md\n"
          ]
        }
      ]
    }
  ]
}